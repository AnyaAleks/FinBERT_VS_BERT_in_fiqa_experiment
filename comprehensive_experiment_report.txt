================================================================================
EXPERIMENT REPORT: BERT vs FinBERT Financial Sentiment Analysis
================================================================================

ğŸ“… Experiment Date: 2025-12-05 09:21:18
ğŸ“Š Test Dataset Size: 6 samples
ğŸ“ˆ Class Distribution: {1: np.int64(3), 0: np.int64(2), 2: np.int64(1)}
   0: Negative, 1: Neutral, 2: Positive

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š MODEL PERFORMANCE RESULTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ·ï¸  BERT:
   Overall Accuracy: 0.5000 (50.00%)
   Weighted Precision: 0.2500
   Weighted Recall: 0.5000
   Weighted F1-Score: 0.3333
   Confusion Matrix Analysis:
     Negative: 0/2 correct (0.0%)
     Neutral: 3/3 correct (100.0%)
     Positive: 0/1 correct (0.0%)

ğŸ·ï¸  FINBERT:
   Overall Accuracy: 0.8333 (83.33%)
   Weighted Precision: 0.8889
   Weighted Recall: 0.8333
   Weighted F1-Score: 0.8333
   Confusion Matrix Analysis:
     Negative: 2/2 correct (100.0%)
     Neutral: 2/3 correct (66.7%)
     Positive: 1/1 correct (100.0%)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ† COMPARATIVE ANALYSIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BERT vs FinBERT Comparison:
  â€¢ BERT Accuracy: 0.5000 (50.00%)
  â€¢ FinBERT Accuracy: 0.8333 (83.33%)
  â€¢ FinBERT Improvement: +0.3333 (+33.33%)
  â€¢ Relative Improvement: 66.7%

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ” KEY FINDINGS AND INTERPRETATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Performance Analysis:
   - FinBERT is specifically pretrained on financial texts
   - BERT is a general-purpose language model
   - Expected: FinBERT should outperform BERT on financial texts

2. Experimental Observations:
   âœ“ FinBERT demonstrates superior financial understanding
   âœ“ Domain-specific pretraining is effective
   âœ“ Financial terminology is better captured by FinBERT

3. Business Implications:
   â€¢ For financial sentiment analysis, use domain-specific models
   â€¢ Consider fine-tuning on company-specific financial language
   â€¢ Regular model updates with recent financial news

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ OUTPUT FILES CREATED
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Visualization Files:
   â€¢ professional_comparison.png - Complete comparison dashboard
   â€¢ bert_vs_finbert.png - Direct accuracy comparison

Data Files:
   â€¢ test_predictions.csv - Detailed predictions

================================================================================